---
layout: single
title:  "Attention"
typora-root-url: ../
categories: study
published: true
tag: [python, study]
toc: true
author_profile: false
sidebar: 
    nav: "docs"
---

# Attention Mechanism 개요
Attention은 기계 번역을 할 때 문장에서 번역하고자 하는 언어의 모든 단어를 주목하는 것이 아니라, 어떤 특정 단어들을 중요하게 취급(주목, Attention)해야 하는지를 결정하기 위하여 개발되었다.
## Attention Map
기존에는 순서가 존재하는 RNN(순차적으로 학습)의 Encoder와 Decoder를 이용하여 진행하였기 때문에 번역하고자 하는 언어의 단어와 번역된 언어의 단어간의 직접적인 관계를 파악하기 어려웠다. Attention은 단어와 단어간의 직접적인 관계를 파악할 수 있으며, Attention Map 은 이것을
시각화 한 그림으로 단어의 흐름을 파악할 수 있다(흰색 -> Attention이 더 많이 들어가 있는 것). 
[https://www.researchgate.net/figure/BERT-Base-attention-maps-for-all-layers-L-0-L-11-Heads-h-0-h-11-in-each-layer-are_fig4_352308310![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/62654997-82aa-4f23-a190-a38e3fcaef5a)](https://forum.opennmt.net/t/extracting-and-visualizing-the-decoder-attention-weights/636)

## Mechanism
https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/20/sockeye_1.gif![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/c4e29025-eca9-46e3-874e-37036f7a5e80)

# Bahdanau Attention 개념

# Luong Attention 개념

# Luong vs Bahdanau

# Attention code


``![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/fa57d048-fcef-41d2-b7dd-3c4aff3b82ff)
`python
# Python program to read
# file word by word

# opening the text file
with open('GFG.txt','r') as file:

	# reading each line
	for line in file:

		# reading each word	
		for word in line.split():

			# displaying the words		
			print(word)
            
```

