---
layout: single
title:  "Attention"
typora-root-url: ../
categories: study
published: true
tag: [python, study]
toc: true
author_profile: false
sidebar: 
    nav: "docs"
---

# Attention Mechanism 개요
Attention은 기계 번역을 할 때 문장에서 번역하고자 하는 언어의 모든 단어를 주목하는 것이 아니라, 어떤 특정 단어들을 중요하게 취급(주목, Attention)해야 하는지를 결정하기 위하여 개발되었다.
## Attention Map
기존에는 순서가 존재하는 RNN(순차적으로 학습)의 Encoder와 Decoder를 이용하여 진행하였기 때문에 번역하고자 하는 언어의 단어와 번역된 언어의 단어간의 직접적인 관계를 파악하기 어려웠다. Attention은 단어와 단어간의 직접적인 관계를 파악할 수 있으며, Attention Map 은 이것을
시각화 한 그림으로 단어의 흐름을 파악할 수 있다(흰색 -> Attention이 더 많이 들어가 있는 것). 

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/fa57d048-fcef-41d2-b7dd-3c4aff3b82ff)
출처: Bahdanau et al. 2015 “Neural machine translation by jointly learning to align and translate”

## Mechanism
기존 모델에서는 Encoder에서 추출된 하나의 vector(전체 문장의 정보가 함축되어 있음)가 Decoder의 hidden state로 작동하였다.
수식적으로는 argmax(p(y|x))로 I am a student라는 문장이 입력되었을 때 Je suis etudiant라는 문장이 나올 확률을 가장 높일 수 있도록 y에 대한 parameter를 최적화하며 학습을 진행한다. end vector 하나로 Decoder를 진행함으로써 발생하는 문제를 long-range dependency problem라고 하는데, 현재의 time step과 멀어지면 멀어질수록 정보의 손실이 커짐을 의미한다. 

이러한 문제점을 해결하기 위하여 Attention 방법에서는 context vector를 만들어 전체 정보를 활용할 수 있도록 보완하였다. 아래 그림에서 볼 수 있듯 I am a student 문장에서의 모든 단어를 할용할 수 있도록 hidden state를 구성한다. 예를 들면 Je라는 단어에서 suis라는 단어를 예측할 때와 그리고 suis라는 단어에서 etudiant라는 단어를 예측할 때마다 context vector를 새롭게 계산하여 사용한다. 

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/7c17817f-3891-4422-9e00-24bbcd1ed6d8)
출처: https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg?hl=ko

Context vector를 계산하는 방법은 다음과 같다.
먼저 Ju라는 단어에 대하여 I, am, a, student 단어들(key)과 attention weight(유사도 개념으로 보통 Bahdanau Attention을 사용)를 계산한다. 계산된 weight를 이용하여 I, am, a, student 단어들(value)과 다시 가중합을 구하고 이를 convext vector로 사용하여 다음에 올 단어가 무엇인지 예측한다. 그리고 이 과정을 반복하여 수행한다.

Attention 방법은 long-range dependency problem을 해결하여 문장이 길더라도 좋은 성능을 보여주었으며, 해석가능한 모델이라는 점에서 그 장점이 있다.

# Bahdanau Attention 개념
[Bahdanau 어텐션](https://d2l.ai/_images/attention-output.svg)
# Luong Attention 개념

# Luong vs Bahdanau

# Attention code


```python
# Python program to read
# file word by word

# opening the text file
with open('GFG.txt','r') as file:

	# reading each line
	for line in file:

		# reading each word	
		for word in line.split():

			# displaying the words		
			print(word)
            
```

